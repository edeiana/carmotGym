# CARMOT Gym

This repository is designed to test CARMOT on benchmark suites.
This repository includes the artifact evaluation materials for the CARMOT CGO 2023 paper: "Program State Element Characterization".

## Artifact

This artifact generates four sets of results that correspond to the main results of the paper shown in Figures 6, 7, 10, 11.
The artifact is a podman image that runs Ubuntu 20.04 and already contains the NAS and PARSEC3 benchmarks suites (we cannot share SPEC CPU 2017 durectly, to include it see the section below in this README.md).

## Prerequisites 

We open sourced [CARMOT](https://github.com/edeiana/carmot.git) and the [infrastructure](https://github.com/edeiana/wholeprogram_benchmarks.git) we built to evaluate CARMOT on several benchmark suites (e.g., NAS, PARSEC3, SPEC CPU 2017).
This artifact will download everything all necessary dependencies by cloning the open-sourced repositories (from GitHub) that are not included within the podman image.
Please make sure to have a network connection when running the artifact.

In order to evaluate this artifact correctly an Intel multicore processor with shared memory is necessary.
The required amount of main memory is 125 GiB to ensure all runs do not go to swap, which can increase the measured execution time of the experiments (this is especially true if you include SPEC2017, as described below).
To ensure the accuracy of execution time measuraments, TurboBoost and HyperThreading have to be disabled and the machine must be idle (no other compute or memory intensive process can run on the machine).
The scripts provided to evaluate this artifact assume that experiments will be run one after the other sequentially, please do not run experiments in parallel or unexpeted behavior might happen.
The required amount of disk space for the whole (fully unpacked) podman image is approximately 200 GB.

## Results

Two sets of results can be generated with this artifact: Minimal and Full.
Results for NAS and PARSEC3 will be generated by default.
If you are allowed to use SPEC2017 for artifact evaluation purposes (please be mindful if your license allows to do so) and you'd like to generate the results for SPEC2017, you will have to include it manually in the image (see Section below to know how).

### Minimal Results
These are the Minimal results that should be evaluated in this artifact.
They consist of four sets:
1) The first set of result corresponds to the black and red speedup bars of Figure 6.
2) The second set is the CARMOT overhead (red bars) in the OpenMP use case of Figure 7.
3) The third set is the CARMOT overhead (red bars) in the C++ Smart Pointer use case of Figure 10.
4) The fourth set is the CARMOT overhead (red bars) in the STATS use case of Figure 11.
Adding SPEC CPU2017 to each set of results is optional.

NOTE:
Computing the Minimal results wihout SPEC2017 takes approximately 2 days.
Adding SPEC2017 increases the time to approximately 4 days.

### Full Results
The Full set of results of the paper consists of the Minimal Results plus the Naive Approach black bars of Figures 7, 10, 11 of the paper.
Adding SPEC CPU2017 to each set of results is optional.

NOTE:
Computing the Full results wihout SPEC2017 takes approximately 4 days.
Adding SPEC2017 increases the time to approximately 6 days.

### How to run the Experiments

To run the experiments do as follows.

Download the artifact (i.e., podman image) following the DOI in the paper appendix.
Load and run the podman image carmot.tar interactively:
$ podman load < carmot.tar
$ podman run --rm -it carmot /bin/bash

This will open a shell inside the podman image.

From inside the podman image, the entry point to generate the Minimal set of results is the script bin/carmot_experiments.
It must be invoked as follows with no arguments:
$ ./bin/carmot_experiments &

Alternatively, the Full set of results can be generated by setting the environment variable CARMOT_FULL to 1.
$ export CARMOT_FULL=1 && ./bin/carmot_experiments &

Additionally, the user can control how many times each data point is executed by setting the environment variable CARMOT_NUM_RUNS to a >0 integer value (the default is CARMOT_NUM_RUNS=3).
For example to generate each data point 5 times the user will invoke bin/carmot_experiments as follows:
$ export CARMOT_NUM_RUNS=5 && ./bin/carmot_experiments &
(Note that the higher the CARMOT_NUM_RUNS, the more time will be needed to compute the results)

The progress of the experiments can be monitored by looking at the ~/carmot_experiments_output.txt:
$ tail -f carmot_experiments_output.txt

The results of the experiments will be placed under results/current_machine in the running podman image.
This directory has the following structure:

results/current_machine/
├── fig10
│   └── carmot
│       ├── NAS
│       │   ├── overhead_blackbars.txt
│       │   └── overhead_redbars.txt
│       ├── PARSEC3
│       │   ├── overhead_blackbars.txt
│       │   └── overhead_redbars.txt
│       └── SPEC2017
│           ├── overhead_blackbars.txt
│           └── overhead_redbars.txt
├── fig11
│   └── carmot
│       └── PARSEC3
│           ├── overhead_blackbars.txt
│           └── overhead_redbars.txt
├── fig6
│   ├── carmot
│   │   ├── NAS
│   │   │   └── speedup.txt
│   │   ├── PARSEC3
│   │   │   └── speedup.txt
│   │   └── SPEC2017
│   │       └── speedup.txt
│   └── original_parallelism
│       ├── NAS
│       │   └── speedup.txt
│       ├── PARSEC3
│       │   ├── speedup_pthread.txt
│       │   └── speedup.txt
│       └── SPEC2017
│           └── speedup.txt
└── fig7
    └── carmot
        ├── NAS
        │   ├── overhead_blackbars.txt
        │   └── overhead_redbars.txt
        ├── PARSEC3
        │   ├── overhead_blackbars.txt
        │   └── overhead_redbars.txt
        └── SPEC2017
            ├── overhead_blackbars.txt
            └── overhead_redbars.txt

NOTE: if CARMOT_FULL is not set, the "overhead_blackbars.txt" files will not be generated.

The directory results/authors_machine contains the results computed by the authors, and follows the same structure.
You can compare your results with the authors results by diff of the corresponding files.
Your results might differ from the authors results depending on how many cores your machine has, how much memory, and whether you successfully disabled TurboBoos and HyperThreading.
If you satisfy the prerequisite listed above in this README.md we expect the trend of your results to be in line to the authors results.

We have NOT tested the execution of these experiments under job scheduling systems like condor or slurm.
We recommend to run these experiments directly on the machine.
Given the amount of time required to run the experiments, if you are running them in a remote machine, we strongly suggest to use tmux or screen to avoid losing the progress made in case the connection is lost.

## Adding SPEC2017 to the podman image
If you are allowed to use SPEC2017 to evaluate this artifact you can do so by opening a new shell on the machine where the podman image is running and getting its container_id using:
$ podman ps

Then, copy your SPEC2017 tar.gz archive from the host to the running podman image using:
$ podman cp /path/to/your/SPEC/archive.tar.gz container_id:~/benchmarkSuites/SPEC2017.tar.gz

Note that your SPEC archive must be a tar.gz archive and the name of the SPEC archive copied into the podman image must be "SPEC2017.tar.gz" .
Your SPEC tar.gz archive must contain a single directory called "SPEC2017" and its structure must be as follows:
SPEC2017/
├── bin
├── cshrc
├── Docs
├── Docs.txt
├── install_archives
├── install.bat
├── install.sh
├── LICENSE
├── LICENSE.txt
├── MANIFEST
├── PTDaemon
├── README
├── README.txt
├── redistributable_sources
├── Revisions
├── shrc
├── shrc.bat
├── tools
├── uninstall.sh
└── version.txt

In order to correctly run the SPEC2017 experiments your SPEC archive must be complete with both the source code of all benchmarks (speed and rate) and all inputs (test, train, reference), otherwise unexpected errors might happen.
Once SPEC2017.tar.gz is added to the podman image, the bin/carmot_experiments script will automatically generate results for SPEC2017 (on top of the already included NAS and PARSEC3).

