# CARMOT Gym

This repository is designed to test CARMOT on well established benchmark suites.
This repository also includes the artifact evaluation materials for the CARMOT CGO 2023 paper: "Program State Element Characterization".

## Artifact

This artifact generates four sets of results that correspond to the main results of the paper.

## Prerequisites 

The artifact is available as a docker image.
We open sourced [CARMOT](https://github.com/edeiana/carmot.git).
We also open sourced the [infrastructure](https://github.com/edeiana/wholeprogram_benchmarks.git) we built to evaluate CARMOT on several benchmark suites (e.g., NAS, PARSEC3, SPEC CPU 2017).
This artifact will download everything is needed by cloning the open-sourced repositories (from GitHub) that are not included within the docker image.
Please make sure to have a network connection when you run the artifact.

In order to evaluate this artifact correctly an Intel multicore processor with shared memory is necessary.
The required amount of main memory is 125 GiB to ensure all runs do not go to swap, which can increase the measured execution time of the experiments (this is especially true if you include SPEC2017, as described below).
To ensure high accuracy of execution time measuraments, TurboBoost and HyperThreading must be disabled and the machine must be idle (no other compute or memory intensive process can run on the machine).
The scripts provided to evaluate this artifact assume that experiments will be run one after the other sequentially, please do not run experiments in parallel or unexpeted behavior might appear.

The required amount of disk space for the whole (fully unpacked) docker image is approximately 200 GB.

## Results

Two sets of results can be generated with this artifact: Minimal and Full.
Results for NAS and PARSEC3 will be generated by default.
Since we cannot share SPEC2017, you will have to include it manually in the image (see Section below to know how).

### Minimal Results
These are the Minimal results that should be evaluated in this artifact.
They consist of four sets:
1) The first set of result corresponds to the black and red speedup bars of Figure 6.
2) The second set is the CARMOT overhead (red bar) in the OpenMP use case of Figure 7.
3) The third set is the CARMOT overhead (red bar) in the C++ Smart Pointer use case of Figure 10.
4) The fourth set is the CARMOT overhead (red bar) in the STATS use case of Figure 11.
Adding SPEC CPU2017 to each set of results is optional.

NOTE:
Computing the Minimal results wihout SPEC2017 takes approximately 2 days.
Adding SPEC2017 increases the time to approximately 4 days.

### Full Results
The Full set of results of the paper consists of the Minimal Results plus the Naive Approach black bars of Figures 7, 10, 11.
Adding SPEC CPU2017 to each set of results is optional.

NOTE:
Computing the Full results wihout SPEC2017 takes approximately 4 days.
Adding SPEC2017 increases the time to approximately 6 days.

### Experiments

To run the experiments do as follows.

Download the artifact (i.e., docker image) following the DOI in the paper appendix.
Load and run the docker image carmot.tar interactively:
$ docker load < carmot.tar
$ docker run --rm -it carmot /bin/bash

This will open a shell inside the docker image.

From inside the docker image, the entry point to generate the Minimal set of results is the script bin/carmot_minimal.
It must be invoked as follows with no arguments:
$ ./bin/carmot_minimal

Alternatively, the Full set of results can be generated by invoking the script bin/carmot_full.
It must be invoked as follows with no arguments:
$ ./bin/carmot_full

Additionally, the user can control how many times each data point is executed by setting the environment variable CARMOT_NUM_RUNS to a >0 integer value (the default is CARMOT_NUM_RUNS=3).
For example to generate each data point 5 times the user will invoke bin/carmot_minimal (or bin/carmot_full) as follows:
$ export CARMOT_NUM_RUNS=5 ; ./bin/carmot_minimal
(Note that the higher the CARMOT_NUM_RUNS, the more time will be needed to compute the results)

Results will be placed under results/current_machine in the running docker image.
This directory has the following structure:

results/current_machine/
├── fig10
│   └── carmot
│       ├── NAS
│       │   ├── overhead_blackbars.txt
│       │   └── overhead_redbars.txt
│       ├── PARSEC3
│       │   ├── overhead_blackbars.txt
│       │   └── overhead_redbars.txt
│       └── SPEC2017
│           ├── overhead_blackbars.txt
│           └── overhead_redbars.txt
├── fig11
│   └── carmot
│       └── PARSEC3
│           ├── overhead_blackbars.txt
│           └── overhead_redbars.txt
├── fig6
│   ├── carmot
│   │   ├── NAS
│   │   │   └── speedup.txt
│   │   ├── PARSEC3
│   │   │   └── speedup.txt
│   │   └── SPEC2017
│   │       └── speedup.txt
│   └── original_parallelism
│       ├── NAS
│       │   └── speedup.txt
│       ├── PARSEC3
│       │   ├── speedup_pthread.txt
│       │   └── speedup.txt
│       └── SPEC2017
│           └── speedup.txt
└── fig7
    └── carmot
        ├── NAS
        │   ├── overhead_blackbars.txt
        │   └── overhead_redbars.txt
        ├── PARSEC3
        │   ├── overhead_blackbars.txt
        │   └── overhead_redbars.txt
        └── SPEC2017
            ├── overhead_blackbars.txt
            └── overhead_redbars.txt

NOTE: if bin/carmot_minimal is executed the "overhead_blackbars.txt" files will not be generated.

The directory results/authors_machine contains the results computed by the authors, and follows the same structure.
You can compare your results with the authors results by diff of the corresponding files.
Your results might differ from the authors results depending on how many cores your machine has, how much memory, and whether you successfully disabled TurboBoos and HyperThreading.
If you satisfy the prerequisite listed above in this README we expect the trend of your results to be in line to the authors results.

## Adding SPEC2017 to the docker image
If you are allowed to use SPEC2017 to evaluate this artifact you can do so by first getting the running container_id using:
$ docker ps

And then copying your SPEC2017 tar.gz archive from the host to the running docker image using:
$ docker cp /absolute/path/to/your/SPEC/archive.tar.gz container_id:~/benchmarkSuites/SPEC2017.tar.gz

Note that your SPEC archive must be a tar.gz archive and the name of the SPEC archive copied into the docker image must be "SPEC2017.tar.gz" .
Your SPEC tar.gz archive must contain a single directory called "SPEC2017" and its structure must be as follows:
SPEC2017/
├── bin
├── cshrc
├── Docs
├── Docs.txt
├── install_archives
├── install.bat
├── install.sh
├── LICENSE
├── LICENSE.txt
├── MANIFEST
├── PTDaemon
├── README
├── README.txt
├── redistributable_sources
├── Revisions
├── shrc
├── shrc.bat
├── tools
├── uninstall.sh
└── version.txt

In order to correctly run the SPEC2017 experiments your SPEC archive must be complete with both source code of all benchmarks (both speed and rate) and all inputs (test, train, reference), otherwise unexpected errors might happen.
Once SPEC2017.tar.gz is added to the docker image, the bin/carmot_minimal and bin/carmot_full scripts will automatically generate results for SPEC2017 (on top of the default NAS and PARSEC3 results).

